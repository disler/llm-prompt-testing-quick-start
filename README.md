# LLM Evaluation & Testing quick start with Promptfoo
> Better, Faster, Cheaper Prompts with LLM Testing & Evaluation

![Fast, Cheap, Accurate](imgs/fast-cheap-accurate-prompt-testing-with-promptfoo.png)

## Setup

- Install promptfoo
  - `npm install -g promptfoo`
  - [Install Docs](https://www.promptfoo.dev/docs/installation)
- To get started with OpenAI, set your OPENAI_API_KEY environment variable.
  - `export OPENAI_API_KEY=<your key>`
- To get started with Gemini, set your VERTEX_API_KEY environment variable.
  - `export VERTEX_API_KEY=<your key>`
  - `export VERTEX_PROJECT_ID=<your google cloud project id>`
  - [Gemini Setup Docs](https://promptfoo.dev/docs/providers/vertex)
- cd into the directory you want to test
- Run `promptfoo eval` to evaluate


*If you want to run OpenAI exclusively comment out other models in the ./\*/promptfooconfig.yaml providers section.*

## Commands

`promptfoo eval` - load and evaluate in the current directory

`promptfoo eval --no-cache` - load and evaluate in the current directory without using the cache

`promptfoo view` - load the UI in the current directory

## Prompt Evaluation Elements
- Providers
- Prompts
- Assertions
- Variables

## Watch the video tutorial
Check out the brief [Video Tutorial](https://youtu.be/KhINc5XwhKs) where we highlight the key features of Promptfoo and how to get started with this repo.

## Use Cases & Value Prop of LLM Testing & Evaluation

- ğŸ’° Save Money & Same Time (Resource Optimization)
  - With LLM testing you cna determine if you need GPT-4 or if you can save money and time with GPT-3
  - You can find the minimum number of tokens you can use without sacrificing quality
  - Compare different LLM providers to determine which is the best fit for your application
- ğŸ‘ Ship with confidence (Validate Accuracy)
  - Gain certainty that your prompt will generate the results you want
  - Confidently generate json responses
  - Compare prompts to determine which is more accurate
- âœ… Prevent Regressions (Consistency)
  - Ensure that the output of a prompt is within the bounds of your expectations
  - Make sure that when you update your prompt it doesn't break your application
  - With version control and CI/CD you can ensure your prompts are always working as expected

## Organizational Pattern
- `/<name of agent 1>`
  - `/prompt.txt` - the prompt(s) to test
  - `/test.yaml` - variables and assertions
  - `/promptfooconfig.yaml` - llm config
- `/<name of agent N>`
  - `...`
- `...`

## Important Docs & Resources
- Vertex Promptfoo Provider
  - https://www.promptfoo.dev/docs/providers/vertex
- Vertex AI Pricing
  - https://cloud.google.com/vertex-ai/pricing
- Great Breakdown of Gemini pro vs gpt-3.5
  - https://klu.ai/blog/gemini-pro-vs-gpt-3-5-turbo
  - https://www.promptfoo.dev/docs/guides/gemini-vs-gpt
- Don't repeat test data
  - https://www.promptfoo.dev/docs/configuration/guide#avoiding-repetition
- Ensure output is in json format and the keys exist
  - https://www.promptfoo.dev/docs/guides/evaluate-json/#ensuring-that-outputs-are-valid-json
- Reference prompt, and test files using globs and lists
  - https://www.promptfoo.dev/docs/configuration/parameters#prompts-from-file
- Assertions ('equals', 'contains', 'is-json', 'levenshtein-distance', 'python', 'regex', 'llm-rubric', and more)
  - https://www.promptfoo.dev/docs/configuration/expected-outputs/
- Example using Scenarios for test assertion variables
  - https://github.com/promptfoo/promptfoo/blob/main/examples/multiple-translations-scenarios/promptfooconfig.yaml
- LLM Providers
  - https://www.promptfoo.dev/docs/providers
- Vertex Provider Src
  - https://github.com/promptfoo/promptfoo/blob/main/src/providers/vertex.ts#L7-L22

## Gemini Pro vs GPT-3.5 Turbo Highlights
> Results generated by GPT-4 and then tweaked - take it with two grains of salt.
- Resources: 
  - Source Blog: https://klu.ai/blog/gemini-pro-vs-gpt-3-5-turbo
  - Vertex Pricing: https://cloud.google.com/vertex-ai/pricing
  - Promptfoo: https://www.promptfoo.dev/docs/guides/gemini-vs-gpt

#### Overall:
- `Gemini-Pro âšªï¸âšªï¸âšªï¸âšªï¸ğŸŸ¢|âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸ GPT-3.5 Turbo`
- Explanation: Gemini-Pro has a small to medium sized edge and is likely a better fit for most applications.

#### Pricing Comparison:
- `Gemini-Pro âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸|âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸ GPT-3.5 Turbo`
- Explanation: Gemini-Pro is the same price as GPT-3.5 Turbo.

#### Speed:
- `Gemini-Pro âšªï¸ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢|âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸ GPT-3.5 Turbo`
- Explanation: Gemini-Pro demonstrates superior speed, processing inputs faster than GPT-3.5 Turbo.

#### Instruction Following:
- `Gemini-Pro âšªï¸âšªï¸ğŸŸ¢ğŸŸ¢ğŸŸ¢|âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸ GPT-3.5 Turbo`
- Explanation: Gemini-Pro excels at following instructions accurately, outperforming GPT-3.5 Turbo in this regard.

#### Content Generation:
- `Gemini-Pro âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸|ğŸŸ¢ğŸŸ¢âšªï¸âšªï¸âšªï¸ GPT-3.5 Turbo`
- Explanation: GPT-3.5 Turbo has a slight advantage in content generation, producing more nuanced and varied outputs.

#### Language Understanding:
- `Gemini-Pro âšªï¸âšªï¸ğŸŸ¢ğŸŸ¢ğŸŸ¢|âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸ GPT-3.5 Turbo`
- Explanation: Gemini-Pro shows superior language understanding, especially in complex comprehension tasks.

#### Bias:
- `Gemini-Pro âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸|ğŸŸ¢ğŸŸ¢ğŸŸ¢âšªï¸âšªï¸ GPT-3.5 Turbo`
- Explanation: Gemini seems to exibit more google specific bias than GPT-3.5 Turbo.

#### API Design and Developer Experience:
- `Gemini-Pro âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸|ğŸŸ¢ğŸŸ¢âšªï¸âšªï¸âšªï¸ GPT-3.5 Turbo`
- Explanation: OpenAIs API is much easier to use than Vertex AI's API.

#### AI Alignment and Safety:
- `Gemini-Pro âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸|ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢âšªï¸ GPT-3.5 Turbo`
- Explanation: Gemini-Pro is extremely restricted in its capabilities.

#### Multimodal Capabilities:
- `Gemini-Pro ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢ğŸŸ¢|âšªï¸âšªï¸âšªï¸âšªï¸âšªï¸ GPT-3.5 Turbo`
- *Explanation: Gemini-Pro supports both text and image inputs, offering a significant advantage over GPT-3.5 Turbo, which is limited to text only.*



## Great LLM Testing & Evaluation Patterns
- Trim your prompts to the minimum number of tokens needed to generate the desired output
- Always compare multiple LLM models to see if you need a expensive, slower model or if a cheaper, faster model will work
- Add as many test assertions as possible to ensure your prompt is generating the output you expect
- Your prompts don't 'always' need to validate every assertions, but they should always validate the most important assertions and most test cases
- Use JSON as the output format for your prompt this makes it easy to validate the output
- Isolate your prompts into separate files for readability and maintainability
- Identify which parts of your prompt are variables and which are static then separate them into different test variables
- Use the `--no-cache` flag to ensure you are always testing the latest version of your prompt
- Use your users as THE primary source for your test cases. Testing every use case your users will encounter is the best way to ensure your prompt is working as expected. This is especially true for prompts that are used in production. 
- Focus on asserting an acceptable range of results over specific, exact results. LLMs are non-deterministic and will generate different results each time. Your tests should account for this.


## Random Idea Fragments

- Google's Bison is not too bad
  - ```- id: vertex:chat-bison
config:
  temperature: 0.5
  max_tokens: 2048
  ```