#!/usr/bin/env node
"use strict";
var __createBinding = (this && this.__createBinding) || (Object.create ? (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    var desc = Object.getOwnPropertyDescriptor(m, k);
    if (!desc || ("get" in desc ? !m.__esModule : desc.writable || desc.configurable)) {
      desc = { enumerable: true, get: function() { return m[k]; } };
    }
    Object.defineProperty(o, k2, desc);
}) : (function(o, m, k, k2) {
    if (k2 === undefined) k2 = k;
    o[k2] = m[k];
}));
var __setModuleDefault = (this && this.__setModuleDefault) || (Object.create ? (function(o, v) {
    Object.defineProperty(o, "default", { enumerable: true, value: v });
}) : function(o, v) {
    o["default"] = v;
});
var __importStar = (this && this.__importStar) || function (mod) {
    if (mod && mod.__esModule) return mod;
    var result = {};
    if (mod != null) for (var k in mod) if (k !== "default" && Object.prototype.hasOwnProperty.call(mod, k)) __createBinding(result, mod, k);
    __setModuleDefault(result, mod);
    return result;
};
var __importDefault = (this && this.__importDefault) || function (mod) {
    return (mod && mod.__esModule) ? mod : { "default": mod };
};
Object.defineProperty(exports, "__esModule", { value: true });
const fs_1 = __importDefault(require("fs"));
const path_1 = __importDefault(require("path"));
const readline_1 = __importDefault(require("readline"));
const chalk_1 = __importDefault(require("chalk"));
const chokidar_1 = __importDefault(require("chokidar"));
const js_yaml_1 = __importDefault(require("js-yaml"));
const commander_1 = require("commander");
const telemetry_1 = __importDefault(require("./telemetry"));
const logger_1 = __importStar(require("./logger"));
const assertions_1 = require("./assertions");
const providers_1 = require("./providers");
const evaluator_1 = require("./evaluator");
const prompts_1 = require("./prompts");
const testCases_1 = require("./testCases");
const util_1 = require("./util");
const onboarding_1 = require("./onboarding");
const cache_1 = require("./cache");
const esm_1 = require("./esm");
const server_1 = require("./web/server");
const updates_1 = require("./updates");
const feedback_1 = require("./feedback");
const list_1 = require("./commands/list");
const show_1 = require("./commands/show");
const table_1 = require("./table");
const share_1 = require("./share");
function createDummyFiles(directory) {
    if (directory) {
        // Make the directory if it doesn't exist
        if (!fs_1.default.existsSync(directory)) {
            fs_1.default.mkdirSync(directory);
        }
    }
    if (directory) {
        if (!fs_1.default.existsSync(directory)) {
            logger_1.default.info(`Creating directory ${directory} ...`);
            fs_1.default.mkdirSync(directory);
        }
    }
    else {
        directory = '.';
    }
    fs_1.default.writeFileSync(path_1.default.join(process.cwd(), directory, 'promptfooconfig.yaml'), onboarding_1.DEFAULT_YAML_CONFIG);
    fs_1.default.writeFileSync(path_1.default.join(process.cwd(), directory, 'README.md'), onboarding_1.DEFAULT_README);
    const isNpx = process.env.npm_execpath?.includes('npx');
    const runCommand = isNpx ? 'npx promptfoo@latest eval' : 'promptfoo eval';
    if (directory === '.') {
        logger_1.default.info(chalk_1.default.green(`✅ Wrote promptfooconfig.yaml. Run \`${chalk_1.default.bold(runCommand)}\` to get started!`));
    }
    else {
        logger_1.default.info(`✅ Wrote promptfooconfig.yaml to ./${directory}`);
        logger_1.default.info(chalk_1.default.green(`Run \`${chalk_1.default.bold(`cd ${directory}`)}\` and then \`${chalk_1.default.bold(runCommand)}\` to get started!`));
    }
}
async function resolveConfigs(cmdObj, defaultConfig) {
    // Config parsing
    let fileConfig = {};
    const configPaths = cmdObj.config;
    if (configPaths) {
        fileConfig = await (0, util_1.readConfigs)(configPaths);
    }
    // Standalone assertion mode
    if (cmdObj.assertions) {
        if (!cmdObj.modelOutputs) {
            logger_1.default.error(chalk_1.default.red('You must provide --model-outputs when using --assertions'));
            process.exit(1);
        }
        const modelOutputs = JSON.parse(fs_1.default.readFileSync(path_1.default.join(process.cwd(), cmdObj.modelOutputs), 'utf8'));
        const assertions = await (0, assertions_1.readAssertions)(cmdObj.assertions);
        fileConfig.prompts = ['{{output}}'];
        fileConfig.providers = ['echo'];
        fileConfig.tests = modelOutputs.map((output) => ({
            vars: {
                output,
            },
            assert: assertions,
        }));
    }
    // Use basepath in cases where path was supplied in the config file
    const basePath = configPaths ? path_1.default.dirname(configPaths[0]) : '';
    const defaultTestRaw = fileConfig.defaultTest || defaultConfig.defaultTest;
    const config = {
        description: fileConfig.description || defaultConfig.description,
        prompts: cmdObj.prompts || fileConfig.prompts || defaultConfig.prompts || [],
        providers: cmdObj.providers || fileConfig.providers || defaultConfig.providers || [],
        tests: cmdObj.tests || cmdObj.vars || fileConfig.tests || defaultConfig.tests || [],
        scenarios: fileConfig.scenarios || defaultConfig.scenarios,
        env: fileConfig.env || defaultConfig.env,
        sharing: process.env.PROMPTFOO_DISABLE_SHARING === '1'
            ? false
            : fileConfig.sharing ?? defaultConfig.sharing ?? true,
        defaultTest: defaultTestRaw ? await (0, testCases_1.readTest)(defaultTestRaw, basePath) : undefined,
        outputPath: cmdObj.output || fileConfig.outputPath || defaultConfig.outputPath,
    };
    // Validation
    if (!config.prompts || config.prompts.length === 0) {
        logger_1.default.error(chalk_1.default.red('You must provide at least 1 prompt'));
        process.exit(1);
    }
    if (!config.providers || config.providers.length === 0) {
        logger_1.default.error(chalk_1.default.red('You must specify at least 1 provider (for example, openai:gpt-3.5-turbo)'));
        process.exit(1);
    }
    // Parse prompts, providers, and tests
    const parsedPrompts = (0, prompts_1.readPrompts)(config.prompts, cmdObj.prompts ? undefined : basePath);
    const parsedProviders = await (0, providers_1.loadApiProviders)(config.providers, {
        env: config.env,
        basePath,
    });
    const parsedTests = await (0, testCases_1.readTests)(config.tests || [], cmdObj.tests ? undefined : basePath);
    // Parse testCases for each scenario
    if (fileConfig.scenarios) {
        for (const scenario of fileConfig.scenarios) {
            const parsedScenarioTests = await (0, testCases_1.readTests)(scenario.tests, cmdObj.tests ? undefined : basePath);
            scenario.tests = parsedScenarioTests;
        }
    }
    const parsedProviderPromptMap = (0, prompts_1.readProviderPromptMap)(config, parsedPrompts);
    if (parsedPrompts.length === 0) {
        logger_1.default.error(chalk_1.default.red('No prompts found'));
        process.exit(1);
    }
    const defaultTest = {
        options: {
            prefix: cmdObj.promptPrefix,
            suffix: cmdObj.promptSuffix,
            provider: cmdObj.grader,
            // rubricPrompt
            ...(config.defaultTest?.options || {}),
        },
        ...config.defaultTest,
    };
    const testSuite = {
        description: config.description,
        prompts: parsedPrompts,
        providers: parsedProviders,
        providerPromptMap: parsedProviderPromptMap,
        tests: parsedTests,
        scenarios: config.scenarios,
        defaultTest,
        nunjucksFilters: (0, util_1.readFilters)(fileConfig.nunjucksFilters || defaultConfig.nunjucksFilters || {}, basePath),
    };
    return { config, testSuite };
}
async function main() {
    await (0, updates_1.checkForUpdates)();
    const pwd = process.cwd();
    const potentialPaths = [
        path_1.default.join(pwd, 'promptfooconfig.js'),
        path_1.default.join(pwd, 'promptfooconfig.json'),
        path_1.default.join(pwd, 'promptfooconfig.yaml'),
    ];
    let defaultConfig = {};
    let defaultConfigPath;
    for (const path of potentialPaths) {
        const maybeConfig = await (0, util_1.maybeReadConfig)(path);
        if (maybeConfig) {
            defaultConfig = maybeConfig;
            defaultConfigPath = path;
            break;
        }
    }
    let evaluateOptions = {};
    if (defaultConfig.evaluateOptions) {
        evaluateOptions.generateSuggestions = defaultConfig.evaluateOptions.generateSuggestions;
        evaluateOptions.maxConcurrency = defaultConfig.evaluateOptions.maxConcurrency;
        evaluateOptions.showProgressBar = defaultConfig.evaluateOptions.showProgressBar;
    }
    const program = new commander_1.Command();
    program.option('--version', 'Print version', () => {
        const packageJson = JSON.parse(fs_1.default.readFileSync(path_1.default.join((0, esm_1.getDirectory)(), '../package.json'), 'utf8'));
        logger_1.default.info(packageJson.version);
    });
    program
        .command('init [directory]')
        .description('Initialize project with dummy files')
        .action(async (directory) => {
        telemetry_1.default.maybeShowNotice();
        createDummyFiles(directory);
        telemetry_1.default.record('command_used', {
            name: 'init',
        });
        await telemetry_1.default.send();
    });
    program
        .command('view [directory]')
        .description('Start browser ui')
        .option('-p, --port <number>', 'Port number', '15500')
        .option('-y, --yes', 'Skip confirmation and auto-open the URL')
        .option('--api-base-url <url>', 'Base URL for viewer API calls')
        .action(async (directory, cmdObj) => {
        telemetry_1.default.maybeShowNotice();
        telemetry_1.default.record('command_used', {
            name: 'view',
        });
        await telemetry_1.default.send();
        if (directory) {
            (0, util_1.setConfigDirectoryPath)(directory);
        }
        (0, server_1.startServer)(cmdObj.port, cmdObj.apiBaseUrl, cmdObj.yes);
    });
    program
        .command('share')
        .description('Create a shareable URL of your most recent eval')
        .option('-y, --yes', 'Skip confirmation')
        .action(async (cmdObj) => {
        telemetry_1.default.maybeShowNotice();
        telemetry_1.default.record('command_used', {
            name: 'share',
        });
        await telemetry_1.default.send();
        const createPublicUrl = async () => {
            const latestResults = (0, util_1.readLatestResults)();
            if (!latestResults) {
                logger_1.default.error('Could not load results. Do you need to run `promptfoo eval` first?');
                process.exit(1);
            }
            const url = await (0, share_1.createShareableUrl)(latestResults.results, latestResults.config);
            logger_1.default.info(`View results: ${chalk_1.default.greenBright.bold(url)}`);
        };
        if (cmdObj.yes || process.env.PROMPTFOO_DISABLE_SHARE_WARNING) {
            createPublicUrl();
        }
        else {
            const reader = readline_1.default.createInterface({
                input: process.stdin,
                output: process.stdout,
            });
            reader.question('Create a private shareable URL of your most recent eval?\n\nTo proceed, please confirm [Y/n] ', async function (answer) {
                if (answer.toLowerCase() !== 'yes' && answer.toLowerCase() !== 'y' && answer !== '') {
                    reader.close();
                    process.exit(1);
                }
                reader.close();
                createPublicUrl();
            });
        }
    });
    program
        .command('cache')
        .description('Manage cache')
        .command('clear')
        .description('Clear cache')
        .action(async () => {
        telemetry_1.default.maybeShowNotice();
        logger_1.default.info('Clearing cache...');
        await (0, cache_1.clearCache)();
        (0, util_1.cleanupOldResults)(0);
        telemetry_1.default.record('command_used', {
            name: 'cache_clear',
        });
        await telemetry_1.default.send();
    });
    program
        .command('feedback [message]')
        .description('Send feedback to the promptfoo developers')
        .action((message) => {
        (0, feedback_1.gatherFeedback)(message);
    });
    program
        .command('generate dataset')
        .description('Generate test cases for a given prompt')
        .option('-i, --instructions [instructions]', 'Additional instructions to follow while generating test cases')
        .option('-c, --config [path]', 'Path to configuration file. Defaults to promptfooconfig.yaml', defaultConfigPath)
        .option('-o, --output [path]', 'Path to output file')
        .option('-w, --write', 'Write results to promptfoo configuration file')
        .option('--numPersonas <number>', 'Number of personas to generate', '5')
        .option('--numTestCasesPerPersona <number>', 'Number of test cases per persona', '3')
        .action(async (_, options) => {
        let testSuite;
        if (options.config) {
            const resolved = await resolveConfigs({
                config: [options.config],
            }, defaultConfig);
            testSuite = resolved.testSuite;
        }
        else {
            throw new Error('Could not find config file. Please use `--config`');
        }
        const results = await (0, testCases_1.synthesizeFromTestSuite)(testSuite, {
            instructions: options.instructions,
            numPersonas: parseInt(options.numPersonas, 10),
            numTestCasesPerPersona: parseInt(options.numTestCasesPerPersona, 10),
        });
        const configAddition = { tests: results.map((result) => ({ vars: result })) };
        const yamlString = js_yaml_1.default.dump(configAddition);
        if (options.output) {
            fs_1.default.writeFileSync(options.output, yamlString);
            (0, util_1.printBorder)();
            logger_1.default.info(`Wrote ${results.length} new test cases to ${options.output}`);
            (0, util_1.printBorder)();
        }
        else {
            (0, util_1.printBorder)();
            logger_1.default.info('New test Cases');
            (0, util_1.printBorder)();
            logger_1.default.info(yamlString);
        }
        (0, util_1.printBorder)();
        const configPath = options.config;
        if (options.write && configPath) {
            const existingConfig = js_yaml_1.default.load(fs_1.default.readFileSync(configPath, 'utf8'));
            existingConfig.tests = [...(existingConfig.tests || []), ...configAddition.tests];
            fs_1.default.writeFileSync(configPath, js_yaml_1.default.dump(existingConfig));
            logger_1.default.info(`Wrote ${results.length} new test cases to ${configPath}`);
        }
        else {
            logger_1.default.info(`Copy the above test cases or run ${chalk_1.default.greenBright('promptfoo generate dataset --write')} to write directly to the config`);
        }
        telemetry_1.default.record('command_used', {
            name: 'generate_dataset',
            numPrompts: testSuite.prompts.length,
            numTestsExisting: (testSuite.tests || []).length,
            numTestsGenerated: results.length,
        });
        await telemetry_1.default.send();
    });
    program
        .command('eval')
        .description('Evaluate prompts')
        .option('-p, --prompts <paths...>', 'Paths to prompt files (.txt)')
        .option('-r, --providers <name or path...>', 'One of: openai:chat, openai:completion, openai:<model name>, or path to custom API caller module')
        .option('-c, --config <paths...>', 'Path to configuration file. Automatically loads promptfooconfig.js/json/yaml')
        .option(
    // TODO(ian): Remove `vars` for v1
    '-v, --vars, -t, --tests <path>', 'Path to CSV with test cases', defaultConfig?.commandLineOptions?.vars)
        .option('-a, --assertions <path>', 'Path to assertions file')
        .option('--model-outputs <path>', 'Path to JSON containing list of LLM output strings')
        .option('-t, --tests <path>', 'Path to CSV with test cases')
        .option('-o, --output <paths...>', 'Path to output file (csv, txt, json, yaml, yml, html)')
        .option('-j, --max-concurrency <number>', 'Maximum number of concurrent API calls', defaultConfig.evaluateOptions?.maxConcurrency
        ? String(defaultConfig.evaluateOptions.maxConcurrency)
        : `${evaluator_1.DEFAULT_MAX_CONCURRENCY}`)
        .option('--repeat <number>', 'Number of times to run each test', defaultConfig.evaluateOptions?.repeat ? String(defaultConfig.evaluateOptions.repeat) : '1')
        .option('--delay <number>', 'Delay between each test (in milliseconds)', defaultConfig.evaluateOptions?.delay ? String(defaultConfig.evaluateOptions.delay) : '0')
        .option('--table-cell-max-length <number>', 'Truncate console table cells to this length', '250')
        .option('--suggest-prompts <number>', 'Generate N new prompts and append them to the prompt list')
        .option('--prompt-prefix <path>', 'This prefix is prepended to every prompt', defaultConfig.defaultTest?.options?.prefix)
        .option('--prompt-suffix <path>', 'This suffix is append to every prompt', defaultConfig.defaultTest?.options?.suffix)
        .option('--no-write', 'Do not write results to promptfoo directory', defaultConfig?.commandLineOptions?.write)
        .option('--no-cache', 'Do not read or write results to disk cache', 
    // TODO(ian): Remove commandLineOptions.cache in v1
    defaultConfig?.commandLineOptions?.cache ?? defaultConfig?.evaluateOptions?.cache)
        .option('--no-progress-bar', 'Do not show progress bar')
        .option('--table', 'Output table in CLI', defaultConfig?.commandLineOptions?.table ?? true)
        .option('--no-table', 'Do not output table in CLI', defaultConfig?.commandLineOptions?.table)
        .option('--share', 'Create a shareable URL', defaultConfig?.commandLineOptions?.share)
        .option('--grader <provider>', 'Model that will grade outputs', defaultConfig?.commandLineOptions?.grader)
        .option('--verbose', 'Show debug logs', defaultConfig?.commandLineOptions?.verbose)
        .option('-w, --watch', 'Watch for changes in config and re-run')
        .action(async (cmdObj) => {
        let config = undefined;
        let testSuite = undefined;
        const runEvaluation = async (initialization) => {
            // Misc settings
            if (cmdObj.verbose) {
                (0, logger_1.setLogLevel)('debug');
            }
            const iterations = parseInt(cmdObj.repeat || '', 10);
            const repeat = !isNaN(iterations) && iterations > 0 ? iterations : 1;
            if (!cmdObj.cache || repeat > 1) {
                logger_1.default.info('Cache is disabled.');
                (0, cache_1.disableCache)();
            }
            ({ config, testSuite } = await resolveConfigs(cmdObj, defaultConfig));
            let maxConcurrency = parseInt(cmdObj.maxConcurrency || '', 10);
            const delay = parseInt(cmdObj.delay || '', 0);
            if (delay > 0) {
                maxConcurrency = 1;
                logger_1.default.info(`Running at concurrency=1 because ${delay}ms delay was requested between API calls`);
            }
            const options = {
                showProgressBar: (0, logger_1.getLogLevel)() === 'debug' ? false : cmdObj.progressBar,
                maxConcurrency: !isNaN(maxConcurrency) && maxConcurrency > 0 ? maxConcurrency : undefined,
                repeat,
                delay: !isNaN(delay) && delay > 0 ? delay : undefined,
                ...evaluateOptions,
            };
            if (cmdObj.grader && testSuite.defaultTest) {
                testSuite.defaultTest.options = testSuite.defaultTest.options || {};
                testSuite.defaultTest.options.provider = await (0, providers_1.loadApiProvider)(cmdObj.grader);
            }
            if (cmdObj.generateSuggestions) {
                options.generateSuggestions = true;
            }
            const summary = await (0, evaluator_1.evaluate)(testSuite, {
                ...options,
                eventSource: 'cli',
            });
            const shareableUrl = cmdObj.share && config.sharing ? await (0, share_1.createShareableUrl)(summary, config) : null;
            if (cmdObj.table && (0, logger_1.getLogLevel)() !== 'debug') {
                // Output CLI table
                const table = (0, table_1.generateTable)(summary, parseInt(cmdObj.tableCellMaxLength || '', 10));
                logger_1.default.info('\n' + table.toString());
                if (summary.table.body.length > 25) {
                    const rowsLeft = summary.table.body.length - 25;
                    logger_1.default.info(`... ${rowsLeft} more row${rowsLeft === 1 ? '' : 's'} not shown ...\n`);
                }
            }
            const { outputPath } = config;
            if (outputPath) {
                // Write output to file
                if (typeof outputPath === 'string') {
                    (0, util_1.writeOutput)(outputPath, summary, config, shareableUrl);
                }
                else if (Array.isArray(outputPath)) {
                    (0, util_1.writeMultipleOutputs)(outputPath, summary, config, shareableUrl);
                }
                logger_1.default.info(chalk_1.default.yellow(`Writing output to ${outputPath}`));
            }
            telemetry_1.default.maybeShowNotice();
            (0, util_1.printBorder)();
            if (!cmdObj.write) {
                logger_1.default.info(`${chalk_1.default.green('✔')} Evaluation complete`);
            }
            else {
                (0, util_1.writeLatestResults)(summary, config);
                if (shareableUrl) {
                    logger_1.default.info(`${chalk_1.default.green('✔')} Evaluation complete: ${shareableUrl}`);
                }
                else {
                    logger_1.default.info(`${chalk_1.default.green('✔')} Evaluation complete.\n`);
                    logger_1.default.info(`» Run ${chalk_1.default.greenBright.bold('promptfoo view')} to use the local web viewer`);
                    logger_1.default.info(`» Run ${chalk_1.default.greenBright.bold('promptfoo share')} to create a shareable URL`);
                    logger_1.default.info(`» This project needs your feedback. What's one thing we can improve? ${chalk_1.default.greenBright.bold('https://forms.gle/YFLgTe1dKJKNSCsU7')}`);
                }
            }
            (0, util_1.printBorder)();
            logger_1.default.info(chalk_1.default.green.bold(`Successes: ${summary.stats.successes}`));
            logger_1.default.info(chalk_1.default.red.bold(`Failures: ${summary.stats.failures}`));
            logger_1.default.info(`Token usage: Total ${summary.stats.tokenUsage.total}, Prompt ${summary.stats.tokenUsage.prompt}, Completion ${summary.stats.tokenUsage.completion}, Cached ${summary.stats.tokenUsage.cached}`);
            telemetry_1.default.record('command_used', {
                name: 'eval',
                watch: Boolean(cmdObj.watch),
            });
            await telemetry_1.default.send();
            if (cmdObj.watch) {
                if (initialization) {
                    const configPaths = (cmdObj.config || [defaultConfigPath]).filter(Boolean);
                    if (!configPaths.length) {
                        logger_1.default.error('Could not locate config file(s) to watch');
                        process.exit(1);
                    }
                    const basePath = path_1.default.dirname(configPaths[0]);
                    const promptPaths = Array.isArray(config.prompts)
                        ? config.prompts
                            .map((p) => p.startsWith('file://')
                            ? path_1.default.resolve(basePath, p.slice('file://'.length))
                            : null)
                            .filter(Boolean)
                        : [];
                    const watchPaths = Array.from(new Set([...configPaths, ...promptPaths]));
                    const watcher = chokidar_1.default.watch(watchPaths, { ignored: /^\./, persistent: true });
                    watcher
                        .on('change', async (path) => {
                        (0, util_1.printBorder)();
                        logger_1.default.info(`File change detected: ${path}`);
                        (0, util_1.printBorder)();
                        await runEvaluation();
                    })
                        .on('error', (error) => logger_1.default.error(`Watcher error: ${error}`))
                        .on('ready', () => watchPaths.forEach((watchPath) => logger_1.default.info(`Watching for file changes on ${watchPath} ...`)));
                }
            }
            else {
                logger_1.default.info('Done.');
                if (summary.stats.failures > 0) {
                    process.exit(100);
                }
            }
        };
        await runEvaluation(true /* initialization */);
    });
    (0, list_1.listCommand)(program);
    (0, show_1.showCommand)(program);
    program.parse(process.argv);
    if (!process.argv.slice(2).length) {
        program.outputHelp();
    }
}
main();
//# sourceMappingURL=main.js.map